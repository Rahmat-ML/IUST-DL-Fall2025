{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT1I_sAwoN1U"
      },
      "source": [
        "#Applying Gradient Perturbation to Convolutional Neural Networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AUCkL7xoUvy"
      },
      "source": [
        "This notebook explores techniques for generating adversarial examples in image classification problems. Specifically, it employs gradient-based perturbation attacks that utilize the loss gradient of input images to identify adversarial modifications.\n",
        "\n",
        "The attacks follow the [Fast Gradient Sign Method (FGSM)](https://arxiv.org/pdf/1412.6572), originally proposed by Goodfellow et al. in 2015."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Oqr-ToKL5IKO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcebaQVzfy4b"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVbuPCal6PO-"
      },
      "source": [
        "We use MNIST's handwritten digit data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    # Hint: Set this boolean flag to specify that you want the training portion of the dataset.\n",
        "    train = #TODO,\n",
        "    transform = ToTensor(),\n",
        "    # Hint: Set this boolean flag to automatically download the data if it's not in the 'root' folder.\n",
        "    download = #TODO\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    # Hint: Now, specify that this is the testing portion of the dataset.\n",
        "    train = #TODO,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "# Hint: Create an iterable to load the training data.\n",
        "# It should process the data in mini-batches of size 64 and shuffle them at every epoch.\n",
        "train_dataloader = #TODO\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "_BE6N8Zz1DmW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we construct a basic convolutional neural network (CNN) architecture."
      ],
      "metadata": {
        "id": "UHvwqZGY5a9L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDjN5wfIgJi4"
      },
      "outputs": [],
      "source": [
        "#Defining the CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=5,\n",
        "                stride=1,\n",
        "                padding=2,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 5, 1, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        output = self.out(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following step, we design a simple architecture for a convolutional neural network (CNN)."
      ],
      "metadata": {
        "id": "ihTugshw5iLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SG9KbYfq7W9L"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dataloader, epochs, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss_accumulate = 0\n",
        "\n",
        "        for (data, target) in train_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # Hint: Before calculating new gradients, you must clear the ones from the previous step.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # Hint: Compute the gradient of the loss with respect to all model parameters. This is the backpropagation step.\n",
        "            #TODO\n",
        "\n",
        "            # Hint: Update the model's weights using the gradients computed in the previous step.\n",
        "            #TODO\n",
        "\n",
        "            train_loss_accumulate += loss.item()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, criterion):\n",
        "    # Hint: Switch the model to evaluation mode. This is important for layers like Dropout and BatchNorm.\n",
        "    #TODO\n",
        "\n",
        "    model.to(device)\n",
        "    test_accuracy = 0\n",
        "\n",
        "    # Hint: Since you are not training, gradient calculations are not needed here.\n",
        "    # Use a torch context manager to disable gradient computation for efficiency.\n",
        "    #TODO\n",
        "    with ____: # replace ____ with the correct context manager\n",
        "        for (data, target) in dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            probs = model(data)\n",
        "\n",
        "            # Hint: From the raw model outputs (probs), find the index (the class) with the highest value for each item in the batch.\n",
        "            labels_pred = #TODO\n",
        "\n",
        "            comp = (labels_pred == target)\n",
        "\n",
        "            # Hint: Calculate the number of correct predictions in the batch and add it to the total.\n",
        "            test_accuracy += #TODO\n",
        "\n",
        "    # Hint: Compute the average accuracy across all batches.\n",
        "    return (test_accuracy / len(dataloader.dataset)).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF74TfuI8CKs"
      },
      "source": [
        "By training this CNN without introducing adversarial examples, we observe that it can reach an impressive test accuracy of about 99% on the relatively simple MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dOlNg8Nw7Dux"
      },
      "outputs": [],
      "source": [
        "model = CNN()\n",
        "\n",
        "# Hint: Choose a loss function suitable for a multi-class classification problem.\n",
        "criterion = #TODO\n",
        "\n",
        "# Hint: Choose an optimization algorithm (e.g., Adam or SGD) and pass the model's parameters to it. Set a learning rate, for example, 0.002.\n",
        "optimizer = #TODO\n",
        "\n",
        "# Hint: Define how many times the model will iterate over the entire training dataset.\n",
        "epochs = #TODO\n",
        "\n",
        "model = train_model(model, train_dataloader, epochs, optimizer, criterion)\n",
        "\n",
        "print(\"accuracy of the trained model on the test data:\", evaluate_model(model, test_dataloader, criterion))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITLST_cA-Tpi"
      },
      "source": [
        "#Gradient-based adversarial attacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suF5VptD-ZAC"
      },
      "outputs": [],
      "source": [
        "def attack(model, input, label, criterion, epsilon):\n",
        "    x = input\n",
        "    x = x.to(device)\n",
        "\n",
        "    # Hint: To generate an attack, you need to calculate the gradient with respect to the input data itself, not just the model weights.\n",
        "    # Enable gradient tracking for the input tensor 'x'.\n",
        "    #TODO\n",
        "\n",
        "    loss = criterion(model(x), label)\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Hint: Calculate the gradients of the loss. This will populate the .grad attribute of the input tensor 'x'.\n",
        "    #TODO\n",
        "\n",
        "    # here, epsilon denotes the stepsize\n",
        "    # Hint: Create the perturbation by finding the sign of the gradient of 'x' and scaling it by epsilon.\n",
        "    perturbation = #TODO\n",
        "\n",
        "    x_perturbed = #TODO\n",
        "\n",
        "    return x_perturbed #the perturbed vector is called x_perturbed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9x0gpMrEnzI"
      },
      "source": [
        "\n",
        "The function above implements a straightforward adversarial gradient attack applicable to any model. Given a model, an input, its label, a loss function, and a perturbation magnitude ε, we (assuming the model is in evaluation mode so its parameters are not tracked by autograd) compute the loss between the model's prediction and the label, then obtain the gradient of that loss with respect to the input.\n",
        "By adding a small perturbation in the direction of this gradient (scaled by ε) we produce a new input that maximizes the loss — an adversarial example that may cause the model to misclassify.\n",
        "As demonstrated below, this technique can drastically reduce test accuracy for the trained model, especially as ε grows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GM5hZCxC9Oak"
      },
      "outputs": [],
      "source": [
        "def test_perturbation_accuracy(model, dataloader, criterion, epsilon):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    adversarial_test_accuracy = 0\n",
        "\n",
        "    for (data, target) in dataloader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # Hint: Use the 'attack' function you defined earlier to create a perturbed version of the input 'data'.\n",
        "        data_perturbed = #TODO\n",
        "\n",
        "        data_perturbed = data_perturbed.to(device)\n",
        "\n",
        "        # Hint: Pass the 'data_perturbed' through the model to get the model's prediction on the adversarial example.\n",
        "        probs = #TODO\n",
        "\n",
        "        labels_pred = torch.argmax(probs, dim =1)\n",
        "        comp = (labels_pred == target)\n",
        "        adversarial_test_accuracy += torch.sum(comp.float()) / comp.shape[0]\n",
        "\n",
        "    return (adversarial_test_accuracy/len(dataloader)).item()\n",
        "\n",
        "# Hint: Test the model's robustness with a small perturbation strength (e.g., 0.05).\n",
        "epsilon = #TODO\n",
        "acc = test_perturbation_accuracy(model, test_dataloader, nn.CrossEntropyLoss(), epsilon)\n",
        "print(\"(1) Considering the perturbation radius\", epsilon,\", The model’s test accuracy on the perturbed data is = \", acc, '\\n')\n",
        "\n",
        "# Hint: Test again with a medium perturbation strength (e.g., 0.1).\n",
        "epsilon = #TODO\n",
        "acc = test_perturbation_accuracy(model, test_dataloader, nn.CrossEntropyLoss(), epsilon)\n",
        "print(\"(2) Considering the perturbation radius\", epsilon,\", The model’s test accuracy on the perturbed data is = \", acc, '\\n')\n",
        "\n",
        "# Hint: Test with a larger perturbation strength (e.g., 0.2) to see how much the accuracy drops.\n",
        "epsilon = #TODO\n",
        "acc = test_perturbation_accuracy(model, test_dataloader, nn.CrossEntropyLoss(), epsilon)\n",
        "print(\"(3) Considering the perturbation radius\", epsilon,\", The model’s test accuracy on the perturbed data is = \", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbdcNCWHFQN3"
      },
      "source": [
        "Let's display a selection of the perturbed images to inspect their effects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K5tHBKVeeqq6"
      },
      "outputs": [],
      "source": [
        "epsilons = [0.05, 0.1, 0.2]\n",
        "examples = []\n",
        "accuracies = []\n",
        "\n",
        "\n",
        "data, target = next(iter(test_dataloader))\n",
        "data = data.to(device)\n",
        "target = target.to(device)\n",
        "# Run test for each epsilon\n",
        "\n",
        "for eps in epsilons:\n",
        "    # Hint: Generate an adversarial example using the current epsilon value 'eps'.\n",
        "    ex = #TODO\n",
        "\n",
        "    probs = model(ex)\n",
        "\n",
        "    # Hint: Find the model's predicted label from the raw output probabilities 'probs'.\n",
        "    labels_pred = #TODO\n",
        "\n",
        "    # Hint: Append the final predicted label to the 'accuracies' list to save it.\n",
        "    accuracies.append(#TODO)\n",
        "\n",
        "    # Hint: Also, append the generated adversarial example 'ex' to the 'examples' list for later visualization.\n",
        "    examples.append(#TODO)\n",
        "\n",
        "cnt = 0\n",
        "\n",
        "plt.figure(figsize=(8,10))\n",
        "for i in range(len(epsilons)):\n",
        "    for j in range(len(examples[i])):\n",
        "        cnt += 1\n",
        "        # Hint: Arrange the plots in a grid. The number of rows is the number of epsilons,\n",
        "        # and the number of columns is the number of examples. 'cnt' is the current plot's index.\n",
        "        plt.subplot(#TODO)\n",
        "\n",
        "        plt.xticks([], [])\n",
        "        plt.yticks([], [])\n",
        "        if j == 0:\n",
        "            plt.ylabel(f\"Eps: {epsilons[i]}\", fontsize=14)\n",
        "\n",
        "        # Hint: To plot a tensor, you must first process it: move it to the CPU, detach it,\n",
        "        # remove extra dimensions (squeeze), and convert it to a NumPy array.\n",
        "        ex = #TODO\n",
        "\n",
        "        orig = target[j]\n",
        "        adv = accuracies[i][j]\n",
        "\n",
        "        # Hint: Set the title for each subplot to show the original label and what the model predicted for the adversarial example.\n",
        "        plt.title(#TODO)\n",
        "\n",
        "        # Hint: Display the processed image 'ex' using a \"gray\" colormap.\n",
        "        plt.imshow(#TODO)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gG1B8WSqLpl"
      },
      "source": [
        "##Building a Robust Model with Adversarial Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0Dv4waEqk8I"
      },
      "source": [
        "\n",
        "The idea is straightforward: expose the model to adversarial examples during training so it learns these perturbations and becomes robust to them. In every training iteration we perturb the current mini‑batch (using the attack function defined above) and feed the perturbed data to the model. This encourages the model to learn resilience.\n",
        "\n",
        "For our robustness training we use ε = 0.1 and later evaluate performance under other perturbation magnitudes. A more thorough procedure could randomly sample ε from a continuous range (e.g., [0, 1]) according to some distribution, exposing the model to attacks of varying strength."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ewYJuo0njRx3"
      },
      "outputs": [],
      "source": [
        "def train_model_adversarial(model, train_dataloader, epochs, optimizer, criterion, epsilon):\n",
        "\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss_accumulate = 0\n",
        "\n",
        "        for (data, target) in train_dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Hint: First, create a perturbed version of the clean 'data' using the attack function.\n",
        "            # Crucially, .detach() this new tensor from the current computation graph before using it for training.\n",
        "            data_perturbed = #TODO\n",
        "\n",
        "            # Hint: Now, get the model's output for the 'data_perturbed', not the original clean data.\n",
        "            output = #TODO\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss_accumulate += loss.item()\n",
        "\n",
        "        print(\"epoch = \", epoch, \" loss = \", train_loss_accumulate/len(train_dataloader))\n",
        "\n",
        "    return model\n",
        "\n",
        "cnn_model = CNN()\n",
        "\n",
        "# Hint: Train a new model using your adversarial training function.\n",
        "# You need to provide an epsilon to be used for generating attacks during the training process (e.g., 0.1).\n",
        "Model = train_model_adversarial(cnn_model, train_dataloader, 2, optim.Adam(cnn_model.parameters(), lr = 0.002), nn.CrossEntropyLoss(), #TODO)\n",
        "\n",
        "print(\"Test accuracy of the trained model:\", evaluate_model(Model, test_dataloader, nn.CrossEntropyLoss()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As illustrated below, the model has become significantly more robust, achieving over 90% accuracy for perturbations with ε ≤ 0.1, the value used during training. In contrast, the original model’s accuracy fell to around 50% under the same attacks.\n",
        "\n",
        "Moreover, even for perturbations larger than those seen during robustness training (ε = 0.2), the new model still performs much better, reaching approximately 75% accuracy compared to only 12% for the original model"
      ],
      "metadata": {
        "id": "jHhrTNIh86GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon = 0.05\n",
        "acc = test_perturbation_accuracy(Model, test_dataloader, nn.CrossEntropyLoss(), epsilon)\n",
        "print(\"(1) Considering the perturbation radius\", epsilon,\", The model’s test accuracy on the perturbed data is = \", acc, '\\n')\n",
        "\n",
        "epsilon = 0.1\n",
        "acc = test_perturbation_accuracy(Model, test_dataloader, nn.CrossEntropyLoss(), epsilon)\n",
        "print(\"(2) Considering the perturbation radius\", epsilon,\", The model’s test accuracy on the perturbed data is = \", acc, '\\n')\n",
        "\n",
        "epsilon = 0.2\n",
        "acc = test_perturbation_accuracy(Model, test_dataloader, nn.CrossEntropyLoss(), epsilon)\n",
        "print(\"(3) Considering the perturbation radius\", epsilon,\", The model’s test accuracy on the perturbed data is = \", acc)"
      ],
      "metadata": {
        "id": "edJjbWjS8CfJ"
      },
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "WiOAbU6I8Mgn"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}