{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Libraries"
      ],
      "metadata": {
        "id": "4wPtPCF_7N0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "MyBWk8a557qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\\n\")"
      ],
      "metadata": {
        "id": "g-1H6Nj559RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Fashion-MNIST dataset"
      ],
      "metadata": {
        "id": "_ScNdNCT7ZBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Specify the mean and standard deviation for Fashion-MNIST normalization.\n",
        "    transforms.Normalize(#TODO)\n",
        "])\n",
        "\n",
        "train_dataset = datasets.FashionMNIST('./data', train=#TODO, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST('./data', train=#TODO, transform=transform)"
      ],
      "metadata": {
        "id": "lA3BHoRF7Xdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a fraction of the training set to induce overfitting.\n",
        "train_size = len(train_dataset) // 7\n",
        "train_indices = torch.randperm(len(train_dataset))[:train_size].tolist()\n",
        "train_subset = Subset(train_dataset, train_indices)"
      ],
      "metadata": {
        "id": "FTFCwadJ5_lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the DataLoader for the training subset"
      ],
      "metadata": {
        "id": "sZYwUBGu7gVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = #TODO (batch_size=64)\n",
        "test_loader =  #TODO (batch_size=1000)\n",
        "\n",
        "class_names = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "print(f\"Dataset: Fashion-MNIST\")\n",
        "print(f\"Training samples: {len(train_subset)} (~15% of full dataset)\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "print(f\"Classes: {', '.join(class_names)}\\n\")"
      ],
      "metadata": {
        "id": "IivX2vDG6DsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. COMPLEX MODEL (prone to overfitting)"
      ],
      "metadata": {
        "id": "fLLPgYEp7nRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ComplexModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "U3ng_jIM6aSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. SIMPLE MODEL (model selection)"
      ],
      "metadata": {
        "id": "13qebuOP7slH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(16 * 14 * 14, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "edkzV50B6cXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. MODEL WITH DROPOUT"
      ],
      "metadata": {
        "id": "9_dqPj1T7ueH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelWithDropout(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.dropout(self.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "NaQIoNUI6eBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. LABEL SMOOTHING LOSS"
      ],
      "metadata": {
        "id": "4bVHRf0_7w2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, n_classes=10, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.n_classes = n_classes\n",
        "        self.confidence = 1.0 - smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        log_probs = torch.log_softmax(pred, dim=1)\n",
        "        # Create the smoothed label distribution.\n",
        "        smooth_labels = #TODO\n",
        "        # Fill the smoothed labels tensor.\n",
        "        #TODO\n",
        "        # Set the confidence value for the correct class.\n",
        "        #TODO\n",
        "        # Calculate the final smoothed loss.\n",
        "        loss = #TODO\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "N7z_C_zZ6f6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and evaluation functions"
      ],
      "metadata": {
        "id": "mes-LHFY7zdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device, l1_lambda=0.0):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero out the gradients.\n",
        "        #TODO\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Add L1 regularization if specified.\n",
        "        if l1_lambda > 0:\n",
        "            l1_norm = #TODO\n",
        "            loss = #TODO\n",
        "\n",
        "        # Perform backpropagation.\n",
        "        #TODO\n",
        "        # Update the model weights.\n",
        "        #TODO\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += #TODO\n",
        "\n",
        "    return #TODO\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            # Get the predicted classes from the outputs.\n",
        "            _, predicted = #TODO\n",
        "            total += labels.size(0)\n",
        "            # Calculate the number of correct predictions.\n",
        "            correct += #TODO\n",
        "\n",
        "    return total_loss / len(loader), 100 * correct / total\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=20, l1_lambda=0.0):\n",
        "    model = model.to(device)\n",
        "    train_losses, test_losses = [], []\n",
        "    train_accs, test_accs = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Call the training and evaluation functions for one epoch.\n",
        "        train_loss, train_acc = #TODO\n",
        "        test_loss, test_acc = #TODO\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        test_accs.append(test_acc)\n",
        "\n",
        "        if (epoch + 1) % 4 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "            print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
        "            print(f\"  Overfitting Gap: {train_acc - test_acc:.2f}%\")\n",
        "\n",
        "    return train_losses, test_losses, train_accs, test_accs"
      ],
      "metadata": {
        "id": "mfE4LpLD6lQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiments"
      ],
      "metadata": {
        "id": "9ABxgH2d73mQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store all results\n",
        "results = {}\n",
        "\n",
        "# Experiment 1: Complex model without regularization\n",
        "print(\"=\" * 70)\n",
        "print(\"EXPERIMENT 1: Complex Model (No Regularization)\")\n",
        "print(\"=\" * 70)\n",
        "model1 = ComplexModel()\n",
        "criterion1 = nn.CrossEntropyLoss()\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
        "results['Complex (No Reg)'] = train_model(model1, train_loader, test_loader, criterion1, optimizer1)\n",
        "\n",
        "# Experiment 2: Simple model (Model Selection)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EXPERIMENT 2: Simple Model (Model Selection)\")\n",
        "print(\"=\" * 70)\n",
        "model2 = SimpleModel()\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
        "results['Simple Model'] = train_model(model2, train_loader, test_loader, criterion2, optimizer2)\n",
        "\n",
        "# Experiment 3: Complex model with Dropout\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EXPERIMENT 3: Complex Model with Dropout (rate=0.5)\")\n",
        "print(\"=\" * 70)\n",
        "# Instantiate the model with a specified dropout rate.\n",
        "model3 = ModelWithDropout(#TODO)\n",
        "criterion3 = nn.CrossEntropyLoss()\n",
        "optimizer3 = optim.Adam(model3.parameters(), lr=0.001)\n",
        "results['Complex + Dropout'] = train_model(model3, train_loader, test_loader, criterion3, optimizer3)\n",
        "\n",
        "# Experiment 4: Complex model with L1 Regularization\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EXPERIMENT 4: Complex Model with L1 Regularization (λ=0.0001)\")\n",
        "print(\"=\" * 70)\n",
        "model4 = ComplexModel()\n",
        "criterion4 = nn.CrossEntropyLoss()\n",
        "optimizer4 = optim.Adam(model4.parameters(), lr=0.001)\n",
        "# Train the model, passing the L1 lambda value.\n",
        "results['Complex + L1'] = train_model(model4, train_loader, test_loader, criterion4, optimizer4, l1_lambda=#TODO)\n",
        "\n",
        "# Experiment 5: Complex model with L2 Regularization\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EXPERIMENT 5: Complex Model with L2 Regularization (λ=0.01)\")\n",
        "print(\"=\" * 70)\n",
        "model5 = ComplexModel()\n",
        "criterion5 = nn.CrossEntropyLoss()\n",
        "# Add L2 regularization (weight_decay) to the optimizer.\n",
        "optimizer5 = optim.Adam(model5.parameters(), lr=0.001, weight_decay=#TODO)\n",
        "results['Complex + L2'] = train_model(model5, train_loader, test_loader, criterion5, optimizer5)\n",
        "\n",
        "# Experiment 6: Complex model with Label Smoothing\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EXPERIMENT 6: Complex Model with Label Smoothing (ε=0.1)\")\n",
        "print(\"=\" * 70)\n",
        "model6 = ComplexModel()\n",
        "# Use the custom Label Smoothing loss function.\n",
        "criterion6 = #TODO\n",
        "optimizer6 = optim.Adam(model6.parameters(), lr=0.001)\n",
        "results['Complex + Label Smoothing'] = train_model(model6, train_loader, test_loader, criterion6, optimizer6)\n",
        "\n",
        "# Experiment 7: L1 + L2 (Elastic Net)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EXPERIMENT 7: Complex Model with L1 + L2 (Elastic Net)\")\n",
        "print(\"=\" * 70)\n",
        "model7 = ComplexModel()\n",
        "criterion7 = nn.CrossEntropyLoss()\n",
        "# Add L2 regularization to the optimizer.\n",
        "optimizer7 = optim.Adam(model7.parameters(), lr=0.001, weight_decay=#TODO)\n",
        "# Train the model, passing the L1 lambda value as well.\n",
        "results['Complex + L1 + L2'] = train_model(model7, train_loader, test_loader, criterion7, optimizer7, l1_lambda=#TODO)\n",
        "\n",
        "# Experiment 8: Combined approach (Best practices)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"EXPERIMENT 8: Combined (Dropout + L2 + Label Smoothing)\")\n",
        "print(\"=\" * 70)\n",
        "model8 = ModelWithDropout(dropout_rate=0.3)\n",
        "criterion8 = LabelSmoothingCrossEntropy(n_classes=10, smoothing=0.1)\n",
        "optimizer8 = optim.Adam(model8.parameters(), lr=0.001, weight_decay=0.01)\n",
        "results['Combined'] = train_model(model8, train_loader, test_loader, criterion8, optimizer8)"
      ],
      "metadata": {
        "id": "pgE0Jm_Y6r_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot results"
      ],
      "metadata": {
        "id": "YVU5pPF38Afv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Training Accuracy\n",
        "for name, (_, _, train_accs, _) in results.items():\n",
        "    axes[0, 0].plot(train_accs, label=name, linewidth=2)\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Accuracy (%)', fontsize=11)\n",
        "axes[0, 0].set_title('Training Accuracy', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].legend(fontsize=9)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Test Accuracy\n",
        "for name, (_, _, _, test_accs) in results.items():\n",
        "    axes[0, 1].plot(test_accs, label=name, linewidth=2)\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=11)\n",
        "axes[0, 1].set_ylabel('Accuracy (%)', fontsize=11)\n",
        "axes[0, 1].set_title('Test Accuracy (Generalization)', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].legend(fontsize=9)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Overfitting Gap\n",
        "for name, (_, _, train_accs, test_accs) in results.items():\n",
        "    # Calculate the gap between training and test accuracy for each epoch.\n",
        "    gap = #TODO\n",
        "    axes[1, 0].plot(gap, label=name, linewidth=2)\n",
        "axes[1, 0].set_xlabel('Epoch', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Accuracy Gap (%)', fontsize=11)\n",
        "axes[1, 0].set_title('Overfitting Gap (Train - Test)', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].legend(fontsize=9)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
        "\n",
        "# Plot 4: Final Performance Comparison\n",
        "names = list(results.keys())\n",
        "final_train = [results[name][2][-1] for name in names]\n",
        "final_test = [results[name][3][-1] for name in names]\n",
        "x = np.arange(len(names))\n",
        "width = 0.35\n",
        "\n",
        "# Create the bar plot for final train and test accuracies.\n",
        "bars1 = #TODO\n",
        "bars2 = #TODO\n",
        "\n",
        "axes[1, 1].set_xlabel('Method', fontsize=11)\n",
        "axes[1, 1].set_ylabel('Accuracy (%)', fontsize=11)\n",
        "axes[1, 1].set_title('Final Performance Comparison', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xticks(x)\n",
        "axes[1, 1].set_xticklabels(names, rotation=45, ha='right', fontsize=9)\n",
        "axes[1, 1].legend(fontsize=10)\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('fashion_mnist_regularization_comparison.png', dpi=150, bbox_inches='tight')\n",
        "print(\"\\n✓ Plot saved as 'fashion_mnist_regularization_comparison.png'\")"
      ],
      "metadata": {
        "id": "VUmXIOM-6zRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "id": "nJB9YUm68COv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FINAL RESULTS SUMMARY (Fashion-MNIST)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Method':<30} | {'Train':<8} | {'Test':<8} | {'Gap':<8} | Params\")\n",
        "print(\"-\" * 70)\n",
        "for name, (_, _, train_accs, test_accs) in results.items():\n",
        "    final_train = train_accs[-1]\n",
        "    final_test = test_accs[-1]\n",
        "    gap = final_train - final_test\n",
        "    print(f\"{name:<30} | {final_train:5.2f}% | {final_test:5.2f}% | {gap:5.2f}% |\")\n",
        "\n",
        "# Best model analysis\n",
        "best_test = max(results.items(), key=lambda x: x[1][3][-1])\n",
        "best_gap = min(results.items(), key=lambda x: x[1][2][-1] - x[1][3][-1])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Best Test Accuracy: {best_test[0]} ({best_test[1][3][-1]:.2f}%)\")\n",
        "print(f\"Smallest Overfitting Gap: {best_gap[0]} ({best_gap[1][2][-1] - best_gap[1][3][-1]:.2f}%)\")"
      ],
      "metadata": {
        "id": "nIiW4KPV48of"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}